{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aBlIeFzrvh"
      },
      "source": [
        "<div style=\"direction:rtl;line-height:300%;\">\n",
        "<font face=\"XB Zar\" size=5>\n",
        "<div align=center>\n",
        "<font face=\"B Titr\" size=5>\n",
        "<p></p><p></p>\n",
        "بسمه تعالی\n",
        "<p></p>\n",
        "</font>\n",
        "<p></p>\n",
        "<font>\n",
        "<br>\n",
        "درس بازیابی پیشرفته اطلاعات\n",
        "<br>\n",
        "مدرس: دکتر بیگی\n",
        "</font>\n",
        "<p></p>\n",
        "<br>\n",
        "<font>\n",
        "<b>تمرین دوم</b>\n",
        "</font>\n",
        "<br>\n",
        "<br>\n",
        "موعد تحویل: ... آبان <br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<font>\n",
        "دانشگاه صنعتی شریف\n",
        "<br>\n",
        "دانشکده مهندسی کامپیوتر\n",
        "<br>\n",
        "<br>\n",
        "</font>\n",
        "</div>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqvof4gxzrvm"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>\n",
        "مقدمه\n",
        "</h1>\n",
        "<p>\n",
        "در این تمرین قصد داریم به مباحث زیر بپردازیم:\n",
        "    <li>مدل‌های برداری</li>\n",
        "    <li>امتیازدهی و ارزیابی سیستم بازیابی</li>\n",
        "    <li>مدل‌های احتمالاتی</li>\n",
        "\n",
        "دیتاست مورد استفاده در این تمرین را می‌توانید در کنار این فایل مشاهده کنید. همچنین لطفا پس از اتمام تمرین یک بار از اول تا آخر نوت‌بوک را اجرا کنید تا مطمئن باشید تمام سل‌ها به درستی کار می‌کنند.\n",
        "\n",
        "کتابخانه‌های مورد نظرتان را هم می‌توانید در اولین سل نوت‌بوک فراخوانی کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x1qFWxBAzrvn"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import List, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPxBipDU0Vpo",
        "outputId": "6d88ed17-1bfe-48a6-c8d6-4ba95278191f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmv1Ghezzrvo"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>1.\n",
        "آماده‌سازی دیتاست\n",
        "</h1>\n",
        "<p>\n",
        "با استفاده از تمرین قبل و عملیات‌هایی که در آنجا برای پیش‌پردازش متون پیاده‌سازی کردید، دیتاست داده شده را لود کنید. قرار است در ادامه با این دیتاست بخش‌های بعدی تمرین را پیاده‌سازی کنید. همچنین در صورتی که اجرای سل‌های بعدی برایتان طول کشید، می‌توانید آن را کوچک کنید. مثلا از ۷۰درصد دیتای آن استفاده کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqeqmCnCzrvo",
        "outputId": "ecf9a352-b545-48f5-e057-e672f9da817f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['title', 'abstract', 'Unnamed: 3', 'Unnamed: 4'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# load the dataset\n",
        "dataset = pd.read_csv(\"data.csv\")\n",
        "dataset.columns[1:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "import spacy\n",
        "from typing import Optional\n",
        "\n",
        "class Preprocessor:\n",
        "    instance:Optional['Preprocessor'] = None\n",
        "    @staticmethod\n",
        "    def get_instance(stopwords_path):\n",
        "      if Preprocessor.instance == None:\n",
        "        Preprocessor.instance = Preprocessor(stopwords_path = None)\n",
        "      return Preprocessor.instance\n",
        "    def __init__(self, stopwords_path):\n",
        "        # Create a variable of stop words.\n",
        "        self.stopwords = nltk.corpus.stopwords.words('english')\n",
        "#         with open(stopwords_path, 'r') as f:\n",
        "#             self.stopwords.extend(f.read().splitlines())\n",
        "\n",
        "        # Load the spacy model.\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        # The main function of the class.\n",
        "        text = self.remove_links(text)\n",
        "        text = self.normalize(text)\n",
        "        words = self.word_tokenize(text)\n",
        "        words = self.remove_stopwords(words)\n",
        "        return words\n",
        "\n",
        "#     def normalize(self, text):\n",
        "#         # Normalize text (lower case, stemming, lemmatization, etc.)\n",
        "#         text = text.lower()\n",
        "#         doc = self.nlp(text)\n",
        "#         words = [token.lemma_ for token in doc]\n",
        "#         return ' '.join(words)\n",
        "\n",
        "    def normalize(self, text):\n",
        "        # Normalize text (lower case, stemming, lemmatization, etc.)\n",
        "        text = text.lower()\n",
        "        text = self.remove_punctuations(text)\n",
        "        text = self.lemmatize(text)\n",
        "        text = self.stemm(text)\n",
        "        return text\n",
        "\n",
        "    def lemmatize(self, text):\n",
        "        # Lemmatize text\n",
        "        doc = self.nlp(text)\n",
        "        text = ' '.join([token.lemma_ for token in doc])\n",
        "        return text\n",
        "\n",
        "    def stemm(self, text):\n",
        "        # Stem text\n",
        "        stemmer = nltk.stem.PorterStemmer()\n",
        "        words = nltk.word_tokenize(text)\n",
        "        text = ' '.join([stemmer.stem(word) for word in words])\n",
        "        return text\n",
        "\n",
        "    def remove_links(self, text):\n",
        "        # Remove links\n",
        "        import re\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        return text\n",
        "\n",
        "    def remove_punctuations(self, text):\n",
        "        # Remove punctuations\n",
        "        import string\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def word_tokenize(self, text):\n",
        "        # Tokenize text\n",
        "        words = nltk.word_tokenize(text)\n",
        "        return words\n",
        "\n",
        "    def remove_stopwords(self, words):\n",
        "        # Remove stopwords\n",
        "        words = [word for word in words if word not in self.stopwords]\n",
        "        return words"
      ],
      "metadata": {
        "id": "m1efr77F0b_X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5lAbrvaHzrvo"
      },
      "outputs": [],
      "source": [
        "# preprocess\n",
        "def preprocess_dataset(dataset, preprocessor):\n",
        "    null_data = dataset.isnull()\n",
        "    for col in dataset.columns[1:]:\n",
        "        for i in range(len(dataset)):\n",
        "            if null_data.loc[i, col]:\n",
        "              continue\n",
        "            else:\n",
        "              dataset.at[i, col] = preprocessor.preprocess(dataset.loc[i, col])\n",
        "\n",
        "p = Preprocessor.get_instance(None)\n",
        "preprocess_dataset(dataset, p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify null values\n",
        "null_data = dataset.iloc[:, 1:].isnull()\n",
        "\n",
        "# Create a dictionary representation of the corpus\n",
        "doc_ids = dataset.iloc[:, 0].tolist()\n",
        "corpus = {doc_id: {col: dataset.at[i, col] for col in dataset.columns[1:] if not null_data.at[i, col]}\n",
        "          for i, doc_id in enumerate(doc_ids)}"
      ],
      "metadata": {
        "id": "neW9_04-1yI_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoFV0ypGzrvo"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h2>1-1.\n",
        "نمایه‌سازی\n",
        "</h2>\n",
        "<p>\n",
        "در این بخش باید برای سامانه یک\n",
        "Positional Index\n",
        "بسازید.\n",
        "<br>\n",
        "با توجه به مواردی که در بخش بعد می‌آید و نیاز به جست‌وجو‌ی مجزا و با امتیازدهی متفاوت بر روی بخش‌های مختلف سند مثل عنوان یا چکیده آن، در این قسمت باید نمایه‌ی مناسب برای امکان جست‌وجو‌ در بخش‌های مختلف را پیاده‌سازی کنید.\n",
        "با استفاده از نمایه‌ی ساخته‌شده باید بتوان شماره‌ی تمامی اسنادی که یک کلمه در آن آمده است و همچنین همه‌ی جایگاه‌های این کلمه در هر بخش از هر سند را پیدا کرد.\n",
        "\n",
        "برای این بخش می‌توانید از نمایه‌ای که در تمرین اول زدید استفاده کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3KzXvcBlzrvo"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def construct_positional_indexes(corpus: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Constructs positional indexes for the processed data by inserting words into a trie\n",
        "    and creating positional indexes and posting lists.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : dict\n",
        "        The processed data.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        The constructed positional indexes.\n",
        "    \"\"\"\n",
        "\n",
        "    target_values = ['title', 'abstract']\n",
        "    positional_index = defaultdict(lambda: {target: defaultdict(lambda: []) for target in target_values})\n",
        "\n",
        "    for doc_id, doc_content in corpus.items():\n",
        "        for target in target_values:\n",
        "            tokens = doc_content.get(target, [])\n",
        "            for i, token in enumerate(tokens):\n",
        "                positional_index[token][target][doc_id].append(i)\n",
        "\n",
        "    return positional_index\n",
        "\n",
        "docs = construct_positional_indexes(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dhi9MtMzrvp"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>2.\n",
        "مدل‌های برداری و احتمالاتی\n",
        "</h1>\n",
        "<p>\n",
        "در این بخش قصد داریم تا با استفاده از مدل‌های برداری و احتمالاتی، دو سیستم بازیابی اطلاعات طراحی کنیم. در نهایت قرار است سیستم‌های طراحی شده در این بخش را مورد ارزیابی قرار دهیم.\n",
        "\n",
        "در مدل‌های برداری ما به ازای هر داک که در اختیار داریم و کوئری ورودی یک بردار در فضا در نظر می‌گیریم. در ادامه برای بدست آوردن میزان ارتباط داک‌ها و کوئری، از معیارهایی مانند ضرب داخلی بردارها استفاده می‌کنیم. در این بخش ابتدا بردارهای مربوط به هرکدام را با استفاده از مقادیر\n",
        "tf, idf\n",
        "می‌سازیم و سپس به سراغ محاسبه امتیاز داک‌ها می‌رویم تا در نهایت بتوانیم با استفاده از این امتیازها داک‌هایی مرتبط‌تر را خروجی دهیم و یک سیستم بازیابی کامل را طراحی کنیم.\n",
        "\n",
        "در مدل‌های احتمالاتی اساس کار محاسبه\n",
        "$P(R | d, q)$\n",
        "است. در این روش می‌خواهیم به نوعی داک‌ها را به صورت مدل‌های احتمالاتی ببینیم. در درس مشاهده کردید که در انتها مدل‌های احتمالاتی هم رفتاری مشابه با مدل‌های برداری دارند. در ادامه یک سیستم مبتنی بر این مدل‌ها و با روش\n",
        "Okapi25\n",
        "طراحی می‌کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpUq88E9zrvp"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h2>2-1.\n",
        "ساخت ماتریس TF-IDF\n",
        "</h2>\n",
        "<p>\n",
        "در این مرحله دو تابع\n",
        "<code dir=\"ltr\">get_tf(token, doc_id)</code>\n",
        "و\n",
        "<code dir=\"ltr\">get_idf(token)</code>\n",
        "را پیاده‌سازی کنید که تابع اول مقدار\n",
        "tf\n",
        "توکن ورودی را در شناسه‌ی موردنظر\n",
        "و تابع دوم مقدار\n",
        "idf\n",
        "توکن ورودی را برروی نمایه‌ی کنونی حساب می‌کند و آن را به عنوان خروجی برمی‌گرداند.\n",
        "</p>\n",
        "</div>\n",
        "\n",
        "$$tf_{t, d} = \\text{Numbers of } t \\text{ in the title of document } d + \\text{Numbers of } t \\text{ in the abstract of document } d$$\n",
        "$$idf_t = \\log\\Biggl(\\frac{\\text{Number of documents}}{\\text{Number of documents that contains } t + 1}\\Biggr)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHVj-rCvzrvp",
        "outputId": "90f4d13a-890f-474b-f646-b8a6cda57fdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def get_tf(token: str, positional_index: dict, doc_id: str, target=None) -> int:\n",
        "    \"\"\"\n",
        "    Retrieves the term frequency (TF) of a given token within a specific document.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    token : str\n",
        "        The token for which to retrieve the term frequency.\n",
        "    positional_index : dict\n",
        "        A positional index that maps tokens to their positions in documents.\n",
        "    doc_id : str\n",
        "        The unique identifier of the document in which to calculate the TF.\n",
        "    target : str, optional\n",
        "        The target (e.g., 'title' or 'abstract') within the document. If None, considers both 'title' and 'abstract'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "        The term frequency (TF) of the given token within the specified document.\n",
        "    \"\"\"\n",
        "\n",
        "    if target:\n",
        "        return len(positional_index.get(token, {}).get(target, {}).get(doc_id, []))\n",
        "\n",
        "    return len(positional_index.get(token, {}).get('abstract', {}).get(doc_id, []) +\n",
        "               positional_index.get(token, {}).get('title', {}).get(doc_id, []))\n",
        "\n",
        "get_tf('deep', docs, '40ea606185b59cd07b456cb1022d64bf41f5538d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk7xAu_ozrvq",
        "outputId": "ea2c69cb-8634-40e5-8ffe-71b9c973b946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4915576597870803"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# import numpy as np\n",
        "\n",
        "def get_idf(token: str, docs= docs) -> float:\n",
        "    \"\"\"\n",
        "    Retrieves the inverse document frequency (IDF) of a given token.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    token : str\n",
        "        The token to retrieve the IDF for.\n",
        "    docs : dict\n",
        "        The document collection containing positional indexes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The IDF of the given token.\n",
        "    \"\"\"\n",
        "\n",
        "    total_documents = len(doc_ids)\n",
        "    doc_occurrences = sum(1 for doc_index in docs[token]['abstract'].keys() | docs[token]['title'].keys())\n",
        "\n",
        "    return np.log(total_documents / (doc_occurrences + 1))\n",
        "\n",
        "get_idf('deep', docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmuIWhIVzrvq"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<p>\n",
        "در مرحله‌ی نهایی تابع\n",
        "<code dir=\"ltr\">generate_tfidf_list(corpus, positional_index)</code>\n",
        "را پیاده‌سازی کنید که با ورودی گرفتن\n",
        "corpus\n",
        "و\n",
        "positional_index\n",
        "مقادیر\n",
        "tf-idf\n",
        "را برای هر\n",
        "token\n",
        "به دست می‌آورد و به شکل یک ماتریس به عنوان خروجی برمی‌گرداند.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KXuRZW_Qzrvq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def generate_tfidf_list(corpus: dict, positional_index: dict) -> list[dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Generates a list of dictionaries representing documents with associated TF-IDF scores.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    corpus : dict\n",
        "        The corpus containing the processed data.\n",
        "    positional_index : dict\n",
        "        The positional index containing the term frequencies and document frequencies.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[dict[str, float]]\n",
        "        The list of dictionaries representing documents with associated TF-IDF scores.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Write the function to generate the list of dictionaries with TF-IDF scores.\n",
        "    all_tf_idf = []\n",
        "\n",
        "    all_tokens = set()\n",
        "    documens_tokens = {}\n",
        "    for doc_id in doc_ids:\n",
        "        documens_tokens[doc_id] = set()\n",
        "        for nn, t in corpus[doc_id].items():\n",
        "            documens_tokens[doc_id] = documens_tokens[doc_id].union(set(t))\n",
        "\n",
        "        all_tokens = all_tokens.union(documens_tokens[doc_id])\n",
        "\n",
        "    idf = {}\n",
        "    for token in all_tokens:\n",
        "        idf[token] = get_idf(token, docs)\n",
        "\n",
        "    for doc_id in doc_ids:\n",
        "        tf_idf = {}\n",
        "\n",
        "        for token in documens_tokens[doc_id]:\n",
        "            tf_idf[token] = idf[token] * get_tf(token, positional_index, doc_id)\n",
        "\n",
        "        all_tf_idf.append(tf_idf)\n",
        "\n",
        "    return all_tf_idf\n",
        "\n",
        "tfidf = generate_tfidf_list(corpus, docs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwRPbiJTzrvq",
        "outputId": "9b663d9d-2aea-4f61-fd55-f662be0565c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carbid          \t8.036735104266644\t(normalized: 0.38308004250088257)\n",
            "sic             \t7.343587923706697\t(normalized: 0.3500404004144708)\n",
            "grind           \t6.9381228155985335\t(normalized: 0.33071344875667147)\n",
            "ultrason        \t6.783972135771275\t(normalized: 0.3233656827529847)\n",
            "vibrat          \t6.021832083724378\t(normalized: 0.28703741763762086)\n",
            "ceram           \t6.021832083724378\t(normalized: 0.28703741763762086)\n",
            "silicon         \t5.957293562586807\t(normalized: 0.2839611162416474)\n",
            "amplitud        \t5.511006459958388\t(normalized: 0.26268833817636966)\n",
            "minor           \t5.2035217602104265\t(normalized: 0.24803173318446375)\n",
            "ground          \t5.146363346370478\t(normalized: 0.24530721292604862)\n",
            "face            \t4.3604344323595665\t(normalized: 0.20784502487631001)\n",
            "surfac          \t3.6547084695927614\t(normalized: 0.1742058009497733)\n",
            "analysi         \t1.7704837726926128\t(normalized: 0.0843921057607259)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def show_document_vector(vector: list[dict[str, float]], doc_index: int) -> None:\n",
        "    \"\"\"\n",
        "    Prints the non-zero weights and corresponding terms for a document represented as a vector.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    vector : list of dict of str:float\n",
        "        The list of document vectors, where each document vector is a dictionary with term weights.\n",
        "    doc_index : int\n",
        "        The document index for which to display the vector.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    document_vector = vector[doc_index]\n",
        "    sorted_terms = sorted(document_vector.items(), key=lambda x: x[1], reverse=True)\n",
        "    length = math.sqrt(sum(weight ** 2 for _, weight in sorted_terms))\n",
        "    normalized = {term: tfidf / length for term, tfidf in sorted_terms}\n",
        "\n",
        "    for term, tfidf in sorted_terms:\n",
        "        print(f'{term.ljust(16)}\\t{tfidf}\\t(normalized: {normalized[term]})')\n",
        "\n",
        "show_document_vector(tfidf, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoT6DQ1pzrvq"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h2>2-2.\n",
        "امتیازدهی به سندها\n",
        "</h2>\n",
        "<p>\n",
        "در این بخش می‌خواهیم تا با سه شیوه‌ی متفاوت، داکیومنت‌ها را امتیازدهی کرده و kتا با بالاترین امتیاز را برگردانیم.\n",
        "روش‌ها به شکل زیر است:\n",
        "\n",
        "- **ltc.lnc:** در این روش از cosine similarity استفاده می‌کنیم و داده‌ها را بر اساس آن مرتّب می‌کنیم. برای متوجّه شدن نحوه‌ی عملکرد این سیستم، می‌توانید به اسلایدهای درس مراجعه کنید. همچنین دقت کنید که لازم است این امتیازدهی را هم به روش\n",
        "term-at-a-time\n",
        "و هم به روش\n",
        "doc-at-a-time\n",
        "محاسبه کنید.\n",
        "\n",
        "- **Okapi25:** برای پیاده‌سازی این روش از تساوی زیر استفاده کنید:\n",
        "</p>\n",
        "</div>\n",
        "<div>\n",
        "<p>\n",
        "\n",
        "$$ RSV_d = \\sum_{t \\in q} idf(t)\\times\\frac{(k_1 + 1)tf(t, d)}{k_1((1 - b) + b\\frac{dl(d)}{avg(dl)}) + tf(t, d)} $$\n",
        "</p>\n",
        "</div>\n",
        "<div dir='rtl'>\n",
        "<p>\n",
        "برای محاسبه این دو معیار، از توابعی که در بخش قبل پیاده‌سازی کردید استفاده کنید.\n",
        "\n",
        "در تابع search که مربوط به جست و جوی پرسمان کلی است، به عنوان ورودی پارامتر پرسمان (query)، روش محاسبه امتیاز (method)، تعداد اسنادی که باید برگردانده شود (max-result-count) را ورودی می گیرید. ورودی وزن\n",
        "(weight)\n",
        "نشان می‌دهد که امتیاز نهایی چه وزنی از امتیاز عنوان و چکیده دارد. به زبان دیگر:\n",
        "</p>\n",
        "</div>\n",
        "<div>\n",
        "<p>\n",
        "\n",
        "$$ final\\_score = weight \\times title\\_score + (1 - weight) \\times abstract\\_score $$\n",
        "</p>\n",
        "</div>\n",
        "<div dir='rtl'>\n",
        "<p>\n",
        "\n",
        "**توجّه کنید** که تابع نوشته شده، صرفاً یک prototype است و تا وقتی که نیازمندی‌های گفته شده را پیاده کنید، تغییر تابع مشکلی ندارد.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_Lave(targets, corpus, doc_ids):\n",
        "  Lave = {}\n",
        "\n",
        "  for target in targets:\n",
        "    section_lengths = []\n",
        "\n",
        "    for doc_id in doc_ids:\n",
        "        if target in corpus[doc_id]:\n",
        "            section_lengths.extend(len(section) for section in corpus[doc_id][target])\n",
        "\n",
        "    Lave[target] = sum(section_lengths) / len(section_lengths) if section_lengths else 0\n",
        "    return Lave\n",
        "\n",
        "# Calculate Lave\n",
        "target_values = ['title', 'abstract']\n",
        "Lave = calculate_Lave(target_values, corpus, doc_ids)\n",
        "\n",
        "# targets = ['title', 'abstract']\n",
        "# # Preprocess\n",
        "# Lave = {}\n",
        "\n",
        "# for target in targets:\n",
        "#     lengths = []\n",
        "\n",
        "#     for doc in doc_ids:\n",
        "#         for _, section in corpus[doc].items():\n",
        "#             lengths.append(len(section))\n",
        "\n",
        "#     Lave[target] = sum(lengths) / len(lengths)"
      ],
      "metadata": {
        "id": "MCWwFZT56AKm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QEPcuuzSzrvq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tat_score(query, docIds, docs):\n",
        "    # Implementation of how to calculate tat score function\n",
        "    query_vector = np.ones(len(query))\n",
        "    query_vector = query_vector/np.linalg.norm(query_vector)\n",
        "    #fds\n",
        "\n",
        "    documents_vectors = defaultdict(lambda: [])\n",
        "\n",
        "    for term in query:\n",
        "        all_document_ids = set(doc_ids)\n",
        "        for doc in docs[term]['title']:\n",
        "            documents_vectors[doc].append((1 + np.log(len(doc))) * get_idf(term))\n",
        "            # * get_idf(term, docs))\n",
        "            all_document_ids.discard(doc)\n",
        "        for doc in all_document_ids:\n",
        "            documents_vectors[doc].append(0)\n",
        "\n",
        "    for doc in documents_vectors:\n",
        "        v = np.asarray(documents_vectors[doc])\n",
        "        if v.any():\n",
        "            v /= np.linalg.norm(v)\n",
        "\n",
        "        documents_vectors[doc] = v\n",
        "\n",
        "    scores = {doc: np.dot(v, query_vector) for doc, v in documents_vectors.items()}\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "def dat_score(target, query, docIds, docs):\n",
        "    # Implementation of how to calculate dat score function\n",
        "    query_vector = np.ones(len(query))\n",
        "    query_vecor /= np.linalg.norm(query_vecor)\n",
        "\n",
        "    documents_vectors = defaultdict(lambda: [])\n",
        "\n",
        "    for doc in doc_ids:\n",
        "        vector = [(1 + np.log(len(docs[term][target][doc]))) * get_idf(term,docs) for term in query]\n",
        "        documents_vectors[doc] = np.asarray(vector)\n",
        "\n",
        "    for doc in documents_vectors:\n",
        "        v = np.asarray(documents_vectors[doc])\n",
        "        if v.any():\n",
        "            v /= np.linalg.norm(v)\n",
        "\n",
        "        documents_vectors[doc] = v\n",
        "\n",
        "    scores = {doc: np.dot(v, query_vecor) for doc, v in documents_vectors.items()}\n",
        "\n",
        "    return scores\n",
        "\n",
        "def okapi_score(target, query, docIds, docs):\n",
        "    targets = ['title', 'abstract']\n",
        "    Lave = {}\n",
        "\n",
        "    for target in targets:\n",
        "      lengths = []\n",
        "\n",
        "    for doc in docIds:\n",
        "        for _, section in corpus[doc].items():\n",
        "            lengths.append(len(section))\n",
        "\n",
        "    Lave[target] = sum(lengths) / len(lengths)\n",
        "\n",
        "    # Implementation of how to calculate okapi score\n",
        "    scores = {}\n",
        "    k1 = 1.2\n",
        "    b = 0.75\n",
        "\n",
        "    for doc in doc_ids:\n",
        "        score = 0\n",
        "        if target in corpus[doc].keys():\n",
        "            for term in query:\n",
        "                tf = len(docs[term][target][doc])\n",
        "                Ld = len(corpus[doc][target])\n",
        "                # print(Lave)\n",
        "                score += get_idf(term) * ((k1 + 1) * tf) / (k1 * (1 - b + b * (Ld / Lave[target])) + tf)\n",
        "\n",
        "        scores[doc] = score\n",
        "\n",
        "    return scores\n",
        "\n",
        "def search(query, max_result_count, method='ltc-lnc-tat', weight=0.5):\n",
        "    \"\"\"\n",
        "    Finds relevant documents to query\n",
        "\n",
        "    Parameters\n",
        "    ---------------------------------------------------------------------------------------------------\n",
        "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
        "                        notice that if max_result_count = -1, then you have to return all docs\n",
        "\n",
        "    method: 'ltc-lnc-tat' or 'ltc-lnc-dat' or 'okapi25'\n",
        "\n",
        "    Returns\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    list\n",
        "    Retrieved documents with snippet\n",
        "    \"\"\"\n",
        "\n",
        "    p = Preprocessor.get_instance(None)\n",
        "    query = p.preprocess(query)\n",
        "\n",
        "    methods = {\n",
        "        'ltc-lnc-tat': tat_score,\n",
        "        'ltc-lnc-dat': dat_score,\n",
        "        'okapi25': okapi_score\n",
        "    }\n",
        "\n",
        "    if method not in methods:\n",
        "        raise ValueError(\"Undefined method\")\n",
        "\n",
        "    calculate_function = methods[method]\n",
        "    target_values = ['title', 'abstract']\n",
        "    scores = {target: defaultdict(lambda: 0) for target in target_values}\n",
        "\n",
        "    #fds\n",
        "    for target in target_values:\n",
        "        if method == \"okapi25\" or method == \"ltc-lnc-dat\":\n",
        "          scores[target] = calculate_function(target, query, doc_ids, docs)\n",
        "        # elif method == \"ltc-lnc-dat\":\n",
        "        #   scores[target] = calculate_function(target, query)\n",
        "        else:\n",
        "          scores[target] = calculate_function(query, doc_ids, docs)\n",
        "\n",
        "    sorted_scores = [x for nn, x in sorted([\n",
        "        (weight * scores[target_values[0]][doc] + (1 - weight) * scores[target_values[1]][doc], doc)\n",
        "        for doc in doc_ids], reverse=True)]\n",
        "\n",
        "    if max_result_count == -1:\n",
        "        return sorted_scores\n",
        "\n",
        "    return sorted_scores[:max_result_count]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVVbS7ezrvr"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>3.\n",
        "مدل‌های زبانی\n",
        "</h1>\n",
        "<p>\n",
        "در این بخش یک سیستم مبتنی بر مدل‌های زبانی پیاده‌سازی می‌کنید. مدل زبانی‌ای که باید استفاده کنید مدل\n",
        "unigram\n",
        "است که می‌توان گفت ساده‌ترین مدل ممکن است. در این مدل ما برای هر داک یک مدل دظر نظر می‌گیریم و احتمال حضور کلمات در مدل یک داک\n",
        "نسبت به هم مستقل هستند. در پیاده‌سازی این بخش دستتان خیلی باز است. مدل‌هایی که تشکیل می‌دهید و نحوه\n",
        "smoothing\n",
        "و همچنین پارامترهای مدل همگی به خودتان برمی‌گردد.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cf_T(targets, docs):\n",
        "  T = 0\n",
        "  cf = defaultdict(lambda: 0)\n",
        "# targets = ['abstract', 'title']\n",
        "\n",
        "  for token, token_data in docs.items():\n",
        "    for target in targets:\n",
        "        for doc in token_data[target]:\n",
        "            cf[token] += len(token_data[target][doc])\n",
        "\n",
        "  T = sum(cf.values())\n",
        "\n",
        "  return cf, T\n",
        "# Calculate cf and T\n",
        "target_values = ['title', 'abstract']\n",
        "cf, T = calculate_cf_T(target_values, docs)"
      ],
      "metadata": {
        "id": "vos_QUSH791w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y7Yq3uYozrvr"
      },
      "outputs": [],
      "source": [
        "def lm_search(query: str, max_result_count: int, smoothing_method='JM'):\n",
        "    \"\"\"\n",
        "    Finds relevant documents to query\n",
        "\n",
        "    Parameters\n",
        "    ---------------------------------------------------------------------------------------------------\n",
        "    max_result_count: Return top 'max_result_count' docs which have the highest scores.\n",
        "                        notice that if max_result_count = -1, then you have to return all docs\n",
        "\n",
        "    Returns\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    list\n",
        "    Retrieved documents with snippet\n",
        "    \"\"\"\n",
        "\n",
        "    if smoothing_method not in ['JM', 'D']:\n",
        "        raise ValueError(\"Undefined smoothing method\")\n",
        "\n",
        "    p = Preprocessor(None)\n",
        "    query = p.preprocess(query)\n",
        "    lamda = 0.5\n",
        "    alpha = 0.1\n",
        "\n",
        "    probs = defaultdict(lambda: 1.0)\n",
        "    priors = defaultdict(lambda: 1.0)\n",
        "\n",
        "    for term in query:\n",
        "        priors[term] = cf[term] / T\n",
        "\n",
        "    for doc in doc_ids:\n",
        "        Ld = sum(len(section) for _, section in corpus[doc].items())\n",
        "        for term in query:\n",
        "            tf = get_tf(term, docs, doc)\n",
        "            if smoothing_method == 'JM':\n",
        "                probs[doc] *= lamda * priors[term] + (1 - lamda) * tf / Ld\n",
        "            else:\n",
        "                probs[doc] *= (tf + alpha * priors[term]) / (Ld + alpha)\n",
        "\n",
        "    sorted_scores = [x for _, x in sorted([(probs[doc], doc) for doc in doc_ids], reverse=True)]\n",
        "\n",
        "    if max_result_count == -1:\n",
        "        return sorted_scores\n",
        "\n",
        "    return sorted_scores[:max_result_count]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqRFVI78zrvr"
      },
      "source": [
        "<div dir='rtl'>\n",
        "<h1>4.\n",
        "ارزیابی عملکرد سامانه\n",
        "</h1>\n",
        "<p>\n",
        "در این بخش معیارهای زیر را برای سیستم‌های طراحی شده پیاده‌سازی کنید. سپس در بخش آخر با فراخوانی توابع پیاده‌سازی شده و ورودی دادن مقادیر موجود در فایل\n",
        "validation.json\n",
        "به عنوان ورودی\n",
        "actual\n",
        "توابع، می‌توانید معیارها را برای سیستم‌های بازیابی خودتان بدست بیاورید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1pkfsSwYzrvr"
      },
      "outputs": [],
      "source": [
        "def mean(a):\n",
        "    return sum(a) / len(a)\n",
        "\n",
        "class Evaluation:\n",
        "    def __init__(self, actual, predict):\n",
        "        self.actual = actual\n",
        "        self.predict = predict\n",
        "\n",
        "    def evaluate(self, require_scores):\n",
        "        \"\"\"\n",
        "        Prints require scores for actual and predicts array\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        require_scores : List[str]\n",
        "            Scores required to be calculated\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        \"\"\"\n",
        "        scores_functions = {'precision': self.precision,\n",
        "                            'recall': self.recall,\n",
        "                            'f1': self.f1,\n",
        "                            'map': self.map_,\n",
        "                            'ndcg': self.ndcg,\n",
        "                            'mrr': self.mrr}\n",
        "\n",
        "        for score in require_scores:\n",
        "            print(f\"{score}: {scores_functions[score]()}\")\n",
        "\n",
        "    def precision(self):\n",
        "        precision = []\n",
        "        for i in range(len(self.actual)):\n",
        "            precision.append(sum(1 for p in list(self.predict[i]) if p in self.actual[i]) / len(self.predict[i]))\n",
        "        return mean(precision)\n",
        "\n",
        "    def recall(self):\n",
        "        recall = []\n",
        "        for i in range(len(self.actual)):\n",
        "            recall.append(sum(1 for a in self.actual[i] if a in self.predict[i]) / len(self.actual[i]))\n",
        "        return mean(recall)\n",
        "\n",
        "    def f1(self):\n",
        "        pr = self.precision()\n",
        "        re = self.recall()\n",
        "        f1_score = 2 * (pr * re) / (pr + re)\n",
        "        return f1_score\n",
        "\n",
        "    def map_(self):\n",
        "        map_scores = []\n",
        "        for i in range(len(self.actual)):\n",
        "            predicted = self.predict[i]\n",
        "            cnt = 0\n",
        "            precisions = []\n",
        "            visited = False\n",
        "            for j, doc in enumerate(predicted):\n",
        "                if doc in self.actual[i]:\n",
        "                    cnt += 1\n",
        "                    precisions.append(cnt / (j + 1))\n",
        "                    visited = True\n",
        "            if visited:\n",
        "                map_scores.append(mean(precisions))\n",
        "            else:\n",
        "                map_scores.append(0)\n",
        "        return mean(map_scores)\n",
        "\n",
        "    def ndcg(self):\n",
        "        def dcg(rel):\n",
        "            return np.sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(rel))\n",
        "\n",
        "        ndcg_scores = []\n",
        "        for i in range(len(self.actual)):\n",
        "            predicted = self.predict[i]\n",
        "            rel = [1 if p in self.actual[i] else 0 for p in predicted]\n",
        "            ideal_rel = [1] * len(self.actual[i])\n",
        "            ndcg_scores.append(dcg(rel) / dcg(ideal_rel))\n",
        "        return mean(ndcg_scores)\n",
        "\n",
        "    def mrr(self):\n",
        "        mrr_scores = []\n",
        "        for i in range(len(self.actual)):\n",
        "            for j, doc in enumerate(self.predict[i]):\n",
        "                if doc in self.actual[i]:\n",
        "                    mrr_scores.append(1.0 / (j + 1))\n",
        "                    break\n",
        "            else:\n",
        "                mrr_scores.append(0)\n",
        "        return mean(mrr_scores)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dimKj9Y6zrvr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "require_scores = [\"precision\", \"recall\", \"f1\", \"map\", \"ndcg\", \"mrr\"]\n",
        "\n",
        "# Read actuals and queries from validation.json file\n",
        "actuals = []\n",
        "queries = []\n",
        "\n",
        "with open(\"validation.json\", 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "queries = list(test_data.keys())\n",
        "actuals = [test_data[query] for query in queries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1et-i8Onzrvs",
        "outputId": "092342df-73d6-4817-b4ae-653373e7cc06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.55\n",
            "recall: 0.55\n",
            "f1: 0.55\n",
            "map: 0.7147321428571428\n",
            "ndcg: 0.5784242620410707\n",
            "mrr: 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d6f4447864d2>:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return np.sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(rel))\n"
          ]
        }
      ],
      "source": [
        "predicts = [search(query, 10, method='ltc-lnc-tat') for query in queries]\n",
        "\n",
        "eval_vsm = Evaluation(actuals, predicts)\n",
        "eval_vsm.evaluate(require_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-fmycHGzrvs",
        "outputId": "e3f99eaa-03e7-4ecf-cf0e-0fb85507271b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.5499999999999999\n",
            "recall: 0.5499999999999999\n",
            "f1: 0.5499999999999999\n",
            "map: 0.6875661375661375\n",
            "ndcg: 0.5796797760974072\n",
            "mrr: 0.7142857142857143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d6f4447864d2>:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return np.sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(rel))\n"
          ]
        }
      ],
      "source": [
        "predicts = [search(query, 10, method='okapi25') for query in queries]\n",
        "\n",
        "eval_bm = Evaluation(actuals, predicts)\n",
        "eval_bm.evaluate(require_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc-QmkdAzrvs",
        "outputId": "96e72dc4-ee0f-4626-940f-88f78ac98f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.6333333333333334\n",
            "recall: 0.6333333333333334\n",
            "f1: 0.6333333333333334\n",
            "map: 0.732796884185773\n",
            "ndcg: 0.6348826605782925\n",
            "mrr: 0.7833333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-d6f4447864d2>:69: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  return np.sum((2**r - 1) / np.log2(idx + 2) for idx, r in enumerate(rel))\n"
          ]
        }
      ],
      "source": [
        "predicts = [lm_search(query, 10) for query in queries]\n",
        "\n",
        "eval_lm = Evaluation(actuals, predicts)\n",
        "eval_lm.evaluate(require_scores)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}